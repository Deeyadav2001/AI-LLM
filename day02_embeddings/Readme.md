## Day 02 â€“ Sentence Embeddings and Semantic Similarity

### Objective
To understand how AI represents text meaning using embeddings and how semantic similarity is measured numerically.

---

### What I Implemented
- Generated sentence embeddings using a pretrained embedding model
- Compared multiple sentences using cosine similarity
- Interpreted similarity scores to understand semantic relationships

---

### Key Concepts Learned
- Embeddings are numerical representations of text meaning
- Semantically similar sentences produce similar vectors
- Cosine similarity measures how close meanings are
- Embeddings are used in search, recommendations, and RAG systems

---

### Technologies Used
- Python
- sentence-transformers
- scikit-learn
- Pretrained model: all-MiniLM-L6-v2

---

### Outcome
Verified that AI can distinguish related and unrelated sentences based on meaning rather than keywords.
